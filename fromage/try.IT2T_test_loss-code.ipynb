{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a66bc991",
   "metadata": {},
   "source": [
    "# FROMAGe Visual Dialog (Text Generation)\n",
    "\n",
    "This is a notebook showcasing the VisDial image-and-text-to-text (IT2T) results from our paper, [Grounding Language Models to Images for Multimodal Generation](https://arxiv.org/abs/2301.13823). This result is reported in Table 2 of the paper. This is the standard [VisDial](https://arxiv.org/abs/1611.08669) evaluation, which measures the ability of models to pick out the correct text answer out of 100 options.\n",
    "\n",
    "At least 18GB of GPU memory is required to run FROMAGe, and it has only been tested on A6000, V100, and 3090 GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "475add8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "from transformers import logging\n",
    "from tqdm import notebook\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from fromage import models\n",
    "from fromage import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e884127",
   "metadata": {},
   "source": [
    "### Load Pretrained FROMAGe Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4646a124",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using HuggingFace AutoFeatureExtractor for openai/clip-vit-large-patch14.\n",
      "Using facebook/opt-6.7b for the language model.\n",
      "Using openai/clip-vit-large-patch14 for the visual model with 1 visual tokens.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ybi530/miniconda3/envs/byy/lib/python3.8/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa96674818a84aada1767ac3a454c5f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing the LM.\n",
      "Initializing embedding for the retrieval token [RET] (id = 50266).\n",
      "Restoring pretrained weights for the visual model.\n",
      "Freezing the VM.\n"
     ]
    }
   ],
   "source": [
    "# Load model used in the paper.\n",
    "model_dir = './fromage_model/'\n",
    "model = models.load_fromage(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8d373b",
   "metadata": {},
   "source": [
    "### VisDial\n",
    "\n",
    "Download the VisDial validation [annotations](https://www.dropbox.com/s/ibs3a0zhw74zisc/visdial_1.0_val.zip?dl=0), the [dense answer annotations](https://www.dropbox.com/s/3knyk09ko4xekmc/visdial_1.0_val_dense_annotations.json?dl=0) (for computing MRR) and the [images](https://www.dropbox.com/s/twmtutniktom7tu/VisualDialog_val2018.zip?dl=0). Extract everything to the `VisualDialog` folder.\n",
    "\n",
    "First, we'll load the annotations, and define the paths to our images and annotations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12f2bb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create prompt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Question: What am I supposed to do If I feel adventurous?'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_path = '/var/scratch/ybi530/data/data/train2014/'\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "## generate prompt\n",
    "df = pd.read_csv('/var/scratch/ybi530/result/no_intention.csv',index_col=None)\n",
    "df.head(3)\n",
    "\n",
    "\n",
    "\n",
    "df = df[['image_url', 'intention', 'target_action', 'action1', 'action2',\n",
    "       'action3', 'action4']]\n",
    "\n",
    "\n",
    "df.head(3)\n",
    "\n",
    "\n",
    "\n",
    "def create_prompt(row):\n",
    "    use_columns = ['target_action', 'action1', 'action2',\n",
    "       'action3', 'action4']\n",
    "    choices = ''\n",
    "    for i in use_columns:\n",
    "        choices += row[i]+'.'\n",
    "    return 'Question: What am I supposed to do '+row['intention']+'?'\n",
    "\n",
    "df['prompt'] = df.apply(create_prompt, axis=1)\n",
    "\n",
    "print('Create prompt')\n",
    "\n",
    "\n",
    "df.loc[:,'target_action'] = df['target_action'].apply(lambda x:x[3:])\n",
    "\n",
    "df.loc[:,'action1'] = df['action1'].apply(lambda x:x[3:])\n",
    "df.loc[:,'action2'] = df['action2'].apply(lambda x:x[3:])\n",
    "df.loc[:,'action3'] = df['action3'].apply(lambda x:x[3:])\n",
    "df.loc[:,'action4'] = df['action4'].apply(lambda x:x[3:])\n",
    "\n",
    "prompt = df.prompt[0]\n",
    "prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a4ae3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pixel_values_from_path(path: str, feature_extractor):\n",
    "    \"\"\"Helper function for getting images pixels from a local path.\"\"\"\n",
    "    img = Image.open(path)\n",
    "    img = img.resize((224, 224))\n",
    "    img = img.convert('RGB')\n",
    "    pixel_values = utils.get_pixel_values_for_model(feature_extractor, img)\n",
    "    if torch.cuda.is_available():\n",
    "        pixel_values = pixel_values.bfloat16()\n",
    "        pixel_values = pixel_values.cuda()\n",
    "    return pixel_values[None, ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944691a6",
   "metadata": {},
   "source": [
    "Then, for each VisDial example, we compute the loss conditioned on the image and the preceding dialogue. We return the option with the lowest loss as the answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84ed3b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "topk = (1)\n",
    "# Number of options in a batch to compute loss for.\n",
    "# If using a GPU with lower VRAM, this may have to be lowered.\n",
    "batch_size = 5\n",
    "ce_loss = torch.nn.CrossEntropyLoss(ignore_index=-100, reduction='none').cuda()\n",
    "all_preds = []\n",
    "all_gt_results = []\n",
    "all_losses = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d20c3c02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc71f3af0aaf4e84aa1aeb29c8675d0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "交叉商 torch.Size([655])\n",
      "reshape torch.Size([5, 12])\n",
      "交叉商 torch.Size([665])\n",
      "reshape torch.Size([5, 16])\n",
      "交叉商 torch.Size([720])\n",
      "reshape torch.Size([5, 15])\n"
     ]
    }
   ],
   "source": [
    "for example_idx in notebook.tqdm(range(3)):\n",
    "    dialog = df.prompt[example_idx]\n",
    "    image_url = df['image_url'][example_idx]\n",
    "#     contexts = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        images = get_pixel_values_from_path(\n",
    "            os.path.join(img_path,image_url),\n",
    "            model.model.feature_extractor)\n",
    "        \n",
    "#         image = Image.open(os.path.join(img_dir, f'VisualDialog_{split}2018_{image_id}.jpg'))\n",
    "#         plt.imshow(image)\n",
    "#         plt.show()\n",
    "        visual_embs = model.model.get_visual_embs(images, mode='captioning')\n",
    "\n",
    "#         for i in range(len(dialog['dialog'])):\n",
    "        caption = '\\n'.join(dialog) + '\\nA: '\n",
    "\n",
    "        # Run through every possible option, and pick the option with the lowest loss (= lowest perplexity)\n",
    "        example_losses = []\n",
    "        # Tokenize the dialogue sequence (as this is the same for all answer choices).\n",
    "        caption_ids = model.model.tokenizer(\n",
    "            caption, add_special_tokens=True, return_tensors=\"pt\").input_ids\n",
    "        caption_ids = caption_ids.to(images.device)\n",
    "        caption_embs = model.model.input_embeddings(caption_ids)  # (N, T, D)\n",
    "        condition_length = visual_embs.shape[1] + caption_embs.shape[1]\n",
    "\n",
    "        all_example_embs = []\n",
    "        all_example_labels = []\n",
    "        \n",
    "        \n",
    "#        \n",
    "        answer_options = [df.action1[example_idx],df.action2[example_idx],df.action3[example_idx],df.action4[example_idx],df.target_action[example_idx]]\n",
    "        for _, ans in enumerate(answer_options):\n",
    "            ans_ids = model.model.tokenizer(ans, add_special_tokens=True, return_tensors=\"pt\").input_ids\n",
    "            ans_ids = ans_ids.to(images.device)\n",
    "            ans_embs = model.model.input_embeddings(ans_ids)\n",
    "            input_embs = torch.cat([\n",
    "                visual_embs,\n",
    "                caption_embs,\n",
    "                ans_embs], dim=1)\n",
    "#                 ???????\n",
    "            labels = torch.cat([\n",
    "                torch.zeros(visual_embs.shape[:-1], device=caption_ids.device, dtype=caption_ids.dtype) - 100,\n",
    "                caption_ids,\n",
    "                ans_ids], dim=1)\n",
    "            assert labels.shape[1] == input_embs.shape[1]\n",
    "\n",
    "            all_example_embs.append(input_embs)\n",
    "            all_example_labels.append(labels)\n",
    "\n",
    "        max_len = max([x.shape[1] for x in all_example_labels])\n",
    "        padded_example_embs = [torch.nn.functional.pad(x, (0, 0, 0, max_len - x.shape[1])) for x in all_example_embs]\n",
    "        padded_example_embs = torch.cat(padded_example_embs, axis=0)\n",
    "\n",
    "        padded_example_labels = [torch.nn.functional.pad(x, (0, max_len - x.shape[1]), value=-100) for x in all_example_labels]\n",
    "        padded_example_labels = torch.cat(padded_example_labels, axis=0)\n",
    "\n",
    "        all_logits = []\n",
    "        batches = int(np.ceil(padded_example_embs.shape[0] / batch_size))\n",
    "        for i in range(batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = start_idx + batch_size\n",
    "\n",
    "            out = model.model.lm(\n",
    "                inputs_embeds=padded_example_embs[start_idx:end_idx, ...],\n",
    "                labels=None,\n",
    "                use_cache=False,\n",
    "                output_hidden_states=True)\n",
    "            all_logits.append(out.logits)\n",
    "\n",
    "#                 logits 100个结果，没个是一个答案的输出\n",
    "        logits = torch.cat(all_logits, dim=0)\n",
    "#     计算交叉商损失重点在这一步\n",
    "        example_losses = ce_loss(logits.reshape((-1, logits.shape[-1])), padded_example_labels.reshape((-1,)))\n",
    "        print('交叉商',example_losses.shape)\n",
    "        example_losses = example_losses.reshape((5, max_len))[:, condition_length:]\n",
    "        print('reshape',example_losses.shape)\n",
    "        example_losses = example_losses.sum(axis=1)#总损失\n",
    "\n",
    "        all_losses.append(example_losses.cpu().float().numpy())\n",
    "        scores = -example_losses\n",
    "#             预测值\n",
    "        _, preds = scores.topk(1)\n",
    "        all_preds.append(preds)\n",
    "#         all_gt_results.append(gt_index)\n",
    "\n",
    "#     with open(save_path, 'wb') as wf:\n",
    "#         np.save(wf, {'all_preds': all_preds, 'all_gt_results': all_gt_results, 'all_losses': all_losses})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9644ef2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ace303a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 4096])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visual_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "910b4107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visual_embs.shape[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "336e0d3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 137, 4096])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6820c85f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 137])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57d20e17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 144, 4096])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_example_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c86ed3b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 144])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_example_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f40dd85e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 144, 50267])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "592d5de7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_losses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6470ca39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fba38e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148c4fb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7af709d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caption_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7379dd2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 4096])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caption_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcc8f95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b7a377",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03f46933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a94574483a140e981f2db128cc6038a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([110.5000, 109.0000, 129.0000,  92.0000,  61.7500], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n",
      "tensor([146.0000, 103.5000, 122.0000, 121.0000,  96.0000], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n",
      "tensor([153.0000, 127.0000,  90.5000, 117.0000,  71.0000], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "for example_idx in notebook.tqdm(range(3)):\n",
    "    dialog = df.prompt[example_idx]\n",
    "    image_url = df['image_url'][example_idx]\n",
    "#     contexts = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        images = get_pixel_values_from_path(\n",
    "            os.path.join(img_path,image_url),\n",
    "            model.model.feature_extractor)\n",
    "        \n",
    "#         image = Image.open(os.path.join(img_dir, f'VisualDialog_{split}2018_{image_id}.jpg'))\n",
    "#         plt.imshow(image)\n",
    "#         plt.show()\n",
    "        visual_embs = model.model.get_visual_embs(images, mode='captioning')\n",
    "\n",
    "#         for i in range(len(dialog['dialog'])):\n",
    "        caption = '\\n'.join(dialog) + '\\nA: '\n",
    "\n",
    "        # Run through every possible option, and pick the option with the lowest loss (= lowest perplexity)\n",
    "        example_losses = []\n",
    "        # Tokenize the dialogue sequence (as this is the same for all answer choices).\n",
    "        caption_ids = model.model.tokenizer(\n",
    "            caption, add_special_tokens=True, return_tensors=\"pt\").input_ids\n",
    "        caption_ids = caption_ids.to(images.device)\n",
    "        caption_embs = model.model.input_embeddings(caption_ids)  # (N, T, D)\n",
    "        condition_length = visual_embs.shape[1] + caption_embs.shape[1]\n",
    "\n",
    "        all_example_embs = []\n",
    "        all_example_labels = []\n",
    "        \n",
    "        answer_options = [df.action1[example_idx],df.action2[example_idx],df.action3[example_idx],df.action4[example_idx],df.target_action[example_idx]]\n",
    "        for _, ans in enumerate(answer_options):\n",
    "            ans_ids = model.model.tokenizer(ans, add_special_tokens=True, return_tensors=\"pt\").input_ids\n",
    "            ans_ids = ans_ids.to(images.device)\n",
    "            ans_embs = model.model.input_embeddings(ans_ids)\n",
    "            input_embs = torch.cat([\n",
    "                visual_embs,\n",
    "                caption_embs,\n",
    "                ans_embs], dim=1)\n",
    "#                 ???????\n",
    "            labels = torch.cat([\n",
    "                torch.zeros(visual_embs.shape[:-1], device=caption_ids.device, dtype=caption_ids.dtype) -100,\n",
    "                caption_ids,\n",
    "                ans_ids], dim=1)\n",
    "            assert labels.shape[1] == input_embs.shape[1]\n",
    "\n",
    "            all_example_embs.append(input_embs)\n",
    "            all_example_labels.append(labels)\n",
    "\n",
    "        max_len = max([x.shape[1] for x in all_example_labels])\n",
    "        padded_example_embs = [torch.nn.functional.pad(x, (0, 0, 0, max_len - x.shape[1])) for x in all_example_embs]\n",
    "        padded_example_embs = torch.cat(padded_example_embs, axis=0)\n",
    "\n",
    "        padded_example_labels = [torch.nn.functional.pad(x, (0, max_len - x.shape[1]), value=-100) for x in all_example_labels]\n",
    "        padded_example_labels = torch.cat(padded_example_labels, axis=0)\n",
    "\n",
    "        all_logits = []\n",
    "        batches = int(np.ceil(padded_example_embs.shape[0] / batch_size))\n",
    "        for i in range(batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = start_idx + batch_size\n",
    "\n",
    "            out = model.model.lm(\n",
    "                inputs_embeds=padded_example_embs[start_idx:end_idx, ...],\n",
    "                labels=None,\n",
    "                use_cache=False,\n",
    "                output_hidden_states=True)\n",
    "            all_logits.append(out.logits)\n",
    "\n",
    "#                 logits 100个结果，没个是一个答案的输出\n",
    "        logits = torch.cat(all_logits, dim=0)\n",
    "#     计算交叉商损失重点在这一步\n",
    "        example_losses = ce_loss(logits.reshape((-1, logits.shape[-1])), padded_example_labels.reshape((-1,)))\n",
    "        example_losses = example_losses.reshape((5, max_len))[:, condition_length:]\n",
    "        example_losses = example_losses.sum(axis=1)#总损失\n",
    "        print(example_losses)\n",
    "\n",
    "        all_losses.append(example_losses.cpu().float().numpy())\n",
    "        scores = -example_losses\n",
    "#             预测值\n",
    "        _, preds = scores.topk(1)\n",
    "        all_preds.append(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "452422d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "condition_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01ecab4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e39db2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9bf92c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf5fedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([(i == 4)[0].cpu().numpy() for i in all_preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ab8088e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'probablity_result.npy'\n",
    "with open(save_path, 'wb') as wf:\n",
    "    np.save(wf, {'all_preds': all_preds, 'all_gt_results': all_gt_results, 'all_losses': all_losses})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d4ff4c92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07963734378828718"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([(i == 0)[0].cpu().numpy() for i in all_preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "58a00dc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08698848321489831"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([(i == 1)[0].cpu().numpy() for i in all_preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b1c25b81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17593727027689293"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([(i == 2)[0].cpu().numpy() for i in all_preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b92b9415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18524871355060035"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([(i == 3)[0].cpu().numpy() for i in all_preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c73f024",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534a4a70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60e47d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf5f974",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb271414",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea039b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e51dfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_example_embs[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe4d5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab934e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d441cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a360e3ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 187, 50267])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "dc311105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 187])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_example_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "368b7560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 187])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_example_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f5c18501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_losses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ec669dd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "12824194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([-21.2500, -23.5000, -23.7500, -24.2500, -24.8750, -25.0000, -25.1250,\n",
       "        -25.2500, -26.2500, -27.0000], device='cuda:0', dtype=torch.bfloat16),\n",
       "indices=tensor([90, 71, 11, 31, 72, 50, 83, 91, 73, 60], device='cuda:0'))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09450c20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f2c487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e507128d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22820650",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aef2a81a",
   "metadata": {},
   "source": [
    "### Computing Results\n",
    "\n",
    "Finally, we can compute NDCG, MRR, and Recall@k:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c0b673c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some classes to help us compute NDCG and MRR.\n",
    "# Modified from https://github.com/batra-mlp-lab/visdial-challenge-starter-pytorch/blob/master/visdialch/metrics.py\n",
    "\n",
    "class NDCG(object):\n",
    "    def __init__(self):\n",
    "        self._ndcg_numerator = 0.0\n",
    "        self._ndcg_denominator = 0.0\n",
    "\n",
    "    def observe(\n",
    "            self, predicted_scores: torch.Tensor, target_relevance: torch.Tensor\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Observe model output scores and target ground truth relevance and\n",
    "        accumulate NDCG metric.\n",
    "        Parameters\n",
    "        ----------\n",
    "        predicted_scores: torch.Tensor\n",
    "            A tensor of shape (batch_size, num_options), because dense\n",
    "            annotations are available for 1 randomly picked round out of 10.\n",
    "        target_relevance: torch.Tensor\n",
    "            A tensor of shape same as predicted scores, indicating ground truth\n",
    "            relevance of each answer option for a particular round.\n",
    "        \"\"\"\n",
    "        predicted_scores = predicted_scores.detach()\n",
    "\n",
    "        # shape: (batch_size, 1, num_options)\n",
    "        predicted_scores = predicted_scores.unsqueeze(1)\n",
    "        predicted_ranks = scores_to_ranks(predicted_scores)\n",
    "\n",
    "        # shape: (batch_size, num_options)\n",
    "        predicted_ranks = predicted_ranks.squeeze(1)\n",
    "        batch_size, num_options = predicted_ranks.size()\n",
    "\n",
    "        k = torch.sum(target_relevance != 0, dim=-1)\n",
    "\n",
    "        # shape: (batch_size, num_options)\n",
    "        _, rankings = torch.sort(predicted_ranks, dim=-1)\n",
    "        # Sort relevance in descending order so highest relevance gets top rnk.\n",
    "        _, best_rankings = torch.sort(\n",
    "            target_relevance, dim=-1, descending=True\n",
    "        )\n",
    "\n",
    "        # shape: (batch_size, )\n",
    "        batch_ndcg = []\n",
    "        for batch_index in range(batch_size):\n",
    "            num_relevant = k[batch_index]\n",
    "            dcg = self._dcg(\n",
    "                rankings[batch_index][:num_relevant],\n",
    "                target_relevance[batch_index],\n",
    "            )\n",
    "            best_dcg = self._dcg(\n",
    "                best_rankings[batch_index][:num_relevant],\n",
    "                target_relevance[batch_index],\n",
    "            )\n",
    "            batch_ndcg.append(dcg / best_dcg)\n",
    "\n",
    "        self._ndcg_denominator += batch_size\n",
    "        self._ndcg_numerator += sum(batch_ndcg)\n",
    "\n",
    "    def _dcg(self, rankings: torch.Tensor, relevance: torch.Tensor):\n",
    "        sorted_relevance = relevance[rankings].cpu().float()\n",
    "        discounts = torch.log2(torch.arange(len(rankings)).float() + 2)\n",
    "        return torch.sum(sorted_relevance / discounts, dim=-1)\n",
    "\n",
    "    def retrieve(self, reset: bool = True, key=\"\"):\n",
    "        if self._ndcg_denominator > 0:\n",
    "            metrics = {\n",
    "                key + \"ndcg\": float(self._ndcg_numerator / self._ndcg_denominator)\n",
    "            }\n",
    "        else:\n",
    "            metrics = {}\n",
    "\n",
    "        if reset:\n",
    "            self.reset()\n",
    "        return metrics\n",
    "\n",
    "    def reset(self):\n",
    "        self._ndcg_numerator = 0.0\n",
    "        self._ndcg_denominator = 0.0\n",
    "        \n",
    "\n",
    "def scores_to_ranks(scores: torch.Tensor):\n",
    "    \"\"\"Convert model output scores into ranks.\"\"\"\n",
    "    batch_size, num_rounds, num_options = scores.size()\n",
    "    scores = scores.view(-1, num_options)\n",
    "\n",
    "    # sort in descending order - largest score gets highest rank\n",
    "    sorted_ranks, ranked_idx = scores.sort(1, descending=True)\n",
    "\n",
    "    # i-th position in ranked_idx specifies which score shall take this\n",
    "    # position but we want i-th position to have rank of score at that\n",
    "    # position, do this conversion\n",
    "    ranks = ranked_idx.clone().fill_(0)\n",
    "    for i in range(ranked_idx.size(0)):\n",
    "        for j in range(num_options):\n",
    "            ranks[i][ranked_idx[i][j]] = j\n",
    "    # convert from 0-99 ranks to 1-100 ranks\n",
    "    ranks += 1\n",
    "    ranks = ranks.view(batch_size, num_rounds, num_options)\n",
    "    return ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15e4c2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(save_path, 'rb') as rf:\n",
    "    all_data = np.load(rf, allow_pickle=True).item()\n",
    "    all_preds = all_data['all_preds']\n",
    "    all_gt_results = all_data['all_gt_results']\n",
    "    all_losses = all_data['all_losses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7686317d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top-k, k=1, acc=0.17573\n",
      "top-k, k=5, acc=0.19971\n",
      "top-k, k=10, acc=0.24414\n",
      "top-k, k=20, acc=0.48309\n",
      "MRR: 0.21997\n",
      "NDCG: 0.16594\n"
     ]
    }
   ],
   "source": [
    "top_k_accuracy = collections.defaultdict(list)\n",
    "mrr_results = []\n",
    "all_ranks = []\n",
    "topk = (1, 5, 10, 20)\n",
    "ndcg = NDCG()\n",
    "\n",
    "assert len(all_preds) == len(all_gt_results)\n",
    "for gt, loss in zip(all_gt_results, all_losses):\n",
    "    scores = -loss\n",
    "    _, preds = torch.tensor(scores).topk(100)\n",
    "    rank = np.where(preds == gt)[0][0] + 1\n",
    "    all_ranks.append(rank)\n",
    "    mrr_results.append(1 / rank)\n",
    "\n",
    "    for k in topk:\n",
    "        acc = gt in preds[:k]\n",
    "        top_k_accuracy[k].append(acc)\n",
    "        \n",
    "dense_mrr = []\n",
    "for i in range(len(dense_data)):\n",
    "    idx = i * 10 + dense_data[i]['round_id']\n",
    "    if idx >= len(all_losses):\n",
    "        break\n",
    "    scores = -torch.tensor(all_losses[idx])[None, :]\n",
    "    relevance = torch.tensor(dense_data[i]['gt_relevance'])[None, :]\n",
    "    ndcg.observe(scores, relevance)\n",
    "    dense_mrr.append(mrr_results[idx])\n",
    "\n",
    "for k in topk:\n",
    "    print(f'top-k, k={k}, acc={np.mean(top_k_accuracy[k]):.5f}')\n",
    "print(f'MRR: {np.mean(mrr_results):.5f}')\n",
    "print(f'NDCG: {ndcg.retrieve(reset=True)[\"ndcg\"]:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1982d6fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
