{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a66bc991",
   "metadata": {},
   "source": [
    "# FROMAGe Visual Dialog (Text Generation)\n",
    "\n",
    "This is a notebook showcasing the VisDial image-and-text-to-text (IT2T) results from our paper, [Grounding Language Models to Images for Multimodal Generation](https://arxiv.org/abs/2301.13823). This result is reported in Table 2 of the paper. This is the standard [VisDial](https://arxiv.org/abs/1611.08669) evaluation, which measures the ability of models to pick out the correct text answer out of 100 options.\n",
    "\n",
    "At least 18GB of GPU memory is required to run FROMAGe, and it has only been tested on A6000, V100, and 3090 GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "475add8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "from transformers import logging\n",
    "from tqdm import notebook\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from fromage import models\n",
    "from fromage import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e884127",
   "metadata": {},
   "source": [
    "### Load Pretrained FROMAGe Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4646a124",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using HuggingFace AutoFeatureExtractor for openai/clip-vit-large-patch14.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ybi530/miniconda3/envs/byy/lib/python3.8/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using facebook/opt-6.7b for the language model.\n",
      "Using openai/clip-vit-large-patch14 for the visual model with 1 visual tokens.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33525b8a3b9e44de952e06bae0694152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing the LM.\n",
      "Initializing embedding for the retrieval token [RET] (id = 50266).\n",
      "Restoring pretrained weights for the visual model.\n",
      "Freezing the VM.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 47.53 GiB total capacity; 11.15 GiB already allocated; 14.94 MiB free; 11.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load model used in the paper.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m model_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./fromage_model/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_fromage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/var/scratch/ybi530/fromage/fromage/models.py:673\u001b[0m, in \u001b[0;36mload_fromage\u001b[0;34m(model_dir)\u001b[0m\n\u001b[1;32m    671\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    672\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mbfloat16()\n\u001b[0;32m--> 673\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;66;03m# Load pretrained linear mappings and [RET] embeddings.\u001b[39;00m\n\u001b[1;32m    676\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(model_ckpt_path)\n",
      "File \u001b[0;32m~/miniconda3/envs/byy/lib/python3.8/site-packages/torch/nn/modules/module.py:905\u001b[0m, in \u001b[0;36mModule.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    889\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    890\u001b[0m \n\u001b[1;32m    891\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 905\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/byy/lib/python3.8/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/byy/lib/python3.8/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 797 (4 times)]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/byy/lib/python3.8/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/byy/lib/python3.8/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/miniconda3/envs/byy/lib/python3.8/site-packages/torch/nn/modules/module.py:905\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    889\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    890\u001b[0m \n\u001b[1;32m    891\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 905\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 47.53 GiB total capacity; 11.15 GiB already allocated; 14.94 MiB free; 11.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Load model used in the paper.\n",
    "model_dir = './fromage_model/'\n",
    "model = models.load_fromage(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8d373b",
   "metadata": {},
   "source": [
    "### VisDial\n",
    "\n",
    "Download the VisDial validation [annotations](https://www.dropbox.com/s/ibs3a0zhw74zisc/visdial_1.0_val.zip?dl=0), the [dense answer annotations](https://www.dropbox.com/s/3knyk09ko4xekmc/visdial_1.0_val_dense_annotations.json?dl=0) (for computing MRR) and the [images](https://www.dropbox.com/s/twmtutniktom7tu/VisualDialog_val2018.zip?dl=0). Extract everything to the `VisualDialog` folder.\n",
    "\n",
    "First, we'll load the annotations, and define the paths to our images and annotations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f2bb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = '/var/scratch/ybi530/data/data/train2014/'\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "## generate prompt\n",
    "df = pd.read_csv('/var/scratch/ybi530/result/no_intention.csv',index_col=None)\n",
    "df.head(3)\n",
    "\n",
    "\n",
    "\n",
    "df = df[['image_url', 'intention', 'target_action', 'action1', 'action2',\n",
    "       'action3', 'action4']]\n",
    "\n",
    "\n",
    "df.head(3)\n",
    "\n",
    "\n",
    "def create_prompt1(row):\n",
    "    return row['intention']+',' \n",
    "\n",
    "\n",
    "df['prompt1'] = df.apply(create_prompt1, axis=1)\n",
    "\n",
    "\n",
    "print('Create prompt')\n",
    "\n",
    "df.loc[:,'target_action'] = df['target_action'].apply(lambda x:x[3:])\n",
    "\n",
    "df.loc[:,'action1'] = df['action1'].apply(lambda x:x[3:])\n",
    "df.loc[:,'action2'] = df['action2'].apply(lambda x:x[3:])\n",
    "df.loc[:,'action3'] = df['action3'].apply(lambda x:x[3:])\n",
    "df.loc[:,'action4'] = df['action4'].apply(lambda x:x[3:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4ae3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pixel_values_from_path(path: str, feature_extractor):\n",
    "    \"\"\"Helper function for getting images pixels from a local path.\"\"\"\n",
    "    img = Image.open(path)\n",
    "    img = img.resize((224, 224))\n",
    "    img = img.convert('RGB')\n",
    "    pixel_values = utils.get_pixel_values_for_model(feature_extractor, img)\n",
    "    if torch.cuda.is_available():\n",
    "        pixel_values = pixel_values.bfloat16()\n",
    "        pixel_values = pixel_values.cuda()\n",
    "    return pixel_values[None, ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944691a6",
   "metadata": {},
   "source": [
    "Then, for each VisDial example, we compute the loss conditioned on the image and the preceding dialogue. We return the option with the lowest loss as the answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ed3b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "topk = (1)\n",
    "# Number of options in a batch to compute loss for.\n",
    "# If using a GPU with lower VRAM, this may have to be lowered.\n",
    "batch_size = 5\n",
    "ce_loss = torch.nn.CrossEntropyLoss(ignore_index=-100, reduction='none').cuda()\n",
    "all_preds = []\n",
    "all_gt_results = []\n",
    "all_losses = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20c3c02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for example_idx in notebook.tqdm(range(len(df))):\n",
    "    dialog = df.prompt[example_idx]\n",
    "    image_url = df['image_url'][example_idx]\n",
    "#     contexts = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        images = get_pixel_values_from_path(\n",
    "            os.path.join(img_path,image_url),\n",
    "            model.model.feature_extractor)\n",
    "        \n",
    "#         image = Image.open(os.path.join(img_dir, f'VisualDialog_{split}2018_{image_id}.jpg'))\n",
    "#         plt.imshow(image)\n",
    "#         plt.show()\n",
    "        visual_embs = model.model.get_visual_embs(images, mode='captioning')\n",
    "\n",
    "#         for i in range(len(dialog['dialog'])):\n",
    "        caption = '\\n'.join(dialog) + '\\nA: '\n",
    "\n",
    "        # Run through every possible option, and pick the option with the lowest loss (= lowest perplexity)\n",
    "        example_losses = []\n",
    "        # Tokenize the dialogue sequence (as this is the same for all answer choices).\n",
    "        caption_ids = model.model.tokenizer(\n",
    "            caption, add_special_tokens=True, return_tensors=\"pt\").input_ids\n",
    "        caption_ids = caption_ids.to(images.device)\n",
    "        caption_embs = model.model.input_embeddings(caption_ids)  # (N, T, D)\n",
    "        condition_length = visual_embs.shape[1] + caption_embs.shape[1]\n",
    "\n",
    "        all_example_embs = []\n",
    "        all_example_labels = []\n",
    "        \n",
    "        answer_options = [df.action1[example_idx],df.action2[example_idx],df.action3[example_idx],df.action4[example_idx],df.target_action[example_idx]]\n",
    "        for _, ans in enumerate(answer_options):\n",
    "            ans_ids = model.model.tokenizer(ans, add_special_tokens=True, return_tensors=\"pt\").input_ids\n",
    "            ans_ids = ans_ids.to(images.device)\n",
    "            ans_embs = model.model.input_embeddings(ans_ids)\n",
    "            input_embs = torch.cat([\n",
    "                visual_embs,\n",
    "                caption_embs,\n",
    "                ans_embs], dim=1)\n",
    "#                 ???????\n",
    "            labels = torch.cat([\n",
    "                torch.zeros(visual_embs.shape[:-1], device=caption_ids.device, dtype=caption_ids.dtype) - 100,\n",
    "                caption_ids,\n",
    "                ans_ids], dim=1)\n",
    "            assert labels.shape[1] == input_embs.shape[1]\n",
    "\n",
    "            all_example_embs.append(input_embs)\n",
    "            all_example_labels.append(labels)\n",
    "\n",
    "        max_len = max([x.shape[1] for x in all_example_labels])\n",
    "        padded_example_embs = [torch.nn.functional.pad(x, (0, 0, 0, max_len - x.shape[1])) for x in all_example_embs]\n",
    "        padded_example_embs = torch.cat(padded_example_embs, axis=0)\n",
    "\n",
    "        padded_example_labels = [torch.nn.functional.pad(x, (0, max_len - x.shape[1]), value=-100) for x in all_example_labels]\n",
    "        padded_example_labels = torch.cat(padded_example_labels, axis=0)\n",
    "\n",
    "        all_logits = []\n",
    "        batches = int(np.ceil(padded_example_embs.shape[0] / batch_size))\n",
    "        for i in range(batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = start_idx + batch_size\n",
    "\n",
    "            out = model.model.lm(\n",
    "                inputs_embeds=padded_example_embs[start_idx:end_idx, ...],\n",
    "                labels=None,\n",
    "                use_cache=False,\n",
    "                output_hidden_states=True)\n",
    "            all_logits.append(out.logits)\n",
    "\n",
    "#                 logits 100个结果，没个是一个答案的输出\n",
    "        logits = torch.cat(all_logits, dim=0)\n",
    "#     计算交叉商损失重点在这一步\n",
    "        example_losses = ce_loss(logits.reshape((-1, logits.shape[-1])), padded_example_labels.reshape((-1,)))\n",
    "        example_losses = example_losses.reshape((5, max_len))[:, condition_length:]\n",
    "        example_losses = example_losses.sum(axis=1)#总损失\n",
    "        print(example_losses)\n",
    "\n",
    "        all_losses.append(example_losses.cpu().float().numpy())\n",
    "        \n",
    "#         scores = -example_losses\n",
    "# #             预测值\n",
    "#         _, preds = scores.topk(1)\n",
    "#         all_preds.append(preds)\n",
    "#         all_gt_results.append(gt_index)\n",
    "\n",
    "#     with open(save_path, 'wb') as wf:\n",
    "#         np.save(wf, {'all_preds': all_preds, 'all_gt_results': all_gt_results, 'all_losses': all_losses})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365cf7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_df = pd.DataFrame(data['all_losses'])\n",
    "\n",
    "loss_df.to_csv('fromage_noinstruction.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2bf5fedd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47218818916932126"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([(i == 4)[0].cpu().numpy() for i in all_preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ab8088e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'probablity_result_noinstruction.npy'\n",
    "with open(save_path, 'wb') as wf:\n",
    "    np.save(wf, {'all_preds': all_preds, 'all_gt_results': all_gt_results, 'all_losses': all_losses})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d4ff4c92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07963734378828718"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([(i == 0)[0].cpu().numpy() for i in all_preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "58a00dc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08698848321489831"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([(i == 1)[0].cpu().numpy() for i in all_preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b1c25b81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17593727027689293"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([(i == 2)[0].cpu().numpy() for i in all_preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b92b9415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18524871355060035"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([(i == 3)[0].cpu().numpy() for i in all_preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c73f024",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "534a4a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d60e47d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('probablity_result.npy',allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebf5f974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>110.5</td>\n",
       "      <td>109.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>61.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>146.0</td>\n",
       "      <td>103.5</td>\n",
       "      <td>122.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>96.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>153.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>90.5</td>\n",
       "      <td>117.0</td>\n",
       "      <td>71.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>143.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>93.5</td>\n",
       "      <td>86.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>145.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>85.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4076</th>\n",
       "      <td>119.0</td>\n",
       "      <td>115.5</td>\n",
       "      <td>159.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>119.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4077</th>\n",
       "      <td>100.0</td>\n",
       "      <td>112.5</td>\n",
       "      <td>114.5</td>\n",
       "      <td>123.5</td>\n",
       "      <td>66.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4078</th>\n",
       "      <td>99.5</td>\n",
       "      <td>73.5</td>\n",
       "      <td>115.0</td>\n",
       "      <td>88.5</td>\n",
       "      <td>188.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4079</th>\n",
       "      <td>154.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>108.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4080</th>\n",
       "      <td>107.5</td>\n",
       "      <td>74.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>86.5</td>\n",
       "      <td>84.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4081 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0      1      2      3       4\n",
       "0     110.5  109.0  129.0   92.0   61.75\n",
       "1     146.0  103.5  122.0  121.0   96.00\n",
       "2     153.0  127.0   90.5  117.0   71.00\n",
       "3     143.0  152.0  148.0   93.5   86.00\n",
       "4     145.0  134.0   90.0  100.0   85.00\n",
       "...     ...    ...    ...    ...     ...\n",
       "4076  119.0  115.5  159.0   91.0  119.00\n",
       "4077  100.0  112.5  114.5  123.5   66.00\n",
       "4078   99.5   73.5  115.0   88.5  188.00\n",
       "4079  154.0  100.0   97.0   90.0  108.50\n",
       "4080  107.5   74.0   71.0   86.5   84.00\n",
       "\n",
       "[4081 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data['all_losses'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb271414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea039b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e51dfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_example_embs[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe4d5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab934e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d441cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a360e3ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 187, 50267])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "dc311105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 187])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_example_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "368b7560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 187])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_example_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f5c18501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_losses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ec669dd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "12824194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([-21.2500, -23.5000, -23.7500, -24.2500, -24.8750, -25.0000, -25.1250,\n",
       "        -25.2500, -26.2500, -27.0000], device='cuda:0', dtype=torch.bfloat16),\n",
       "indices=tensor([90, 71, 11, 31, 72, 50, 83, 91, 73, 60], device='cuda:0'))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09450c20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f2c487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e507128d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22820650",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aef2a81a",
   "metadata": {},
   "source": [
    "### Computing Results\n",
    "\n",
    "Finally, we can compute NDCG, MRR, and Recall@k:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c0b673c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some classes to help us compute NDCG and MRR.\n",
    "# Modified from https://github.com/batra-mlp-lab/visdial-challenge-starter-pytorch/blob/master/visdialch/metrics.py\n",
    "\n",
    "class NDCG(object):\n",
    "    def __init__(self):\n",
    "        self._ndcg_numerator = 0.0\n",
    "        self._ndcg_denominator = 0.0\n",
    "\n",
    "    def observe(\n",
    "            self, predicted_scores: torch.Tensor, target_relevance: torch.Tensor\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Observe model output scores and target ground truth relevance and\n",
    "        accumulate NDCG metric.\n",
    "        Parameters\n",
    "        ----------\n",
    "        predicted_scores: torch.Tensor\n",
    "            A tensor of shape (batch_size, num_options), because dense\n",
    "            annotations are available for 1 randomly picked round out of 10.\n",
    "        target_relevance: torch.Tensor\n",
    "            A tensor of shape same as predicted scores, indicating ground truth\n",
    "            relevance of each answer option for a particular round.\n",
    "        \"\"\"\n",
    "        predicted_scores = predicted_scores.detach()\n",
    "\n",
    "        # shape: (batch_size, 1, num_options)\n",
    "        predicted_scores = predicted_scores.unsqueeze(1)\n",
    "        predicted_ranks = scores_to_ranks(predicted_scores)\n",
    "\n",
    "        # shape: (batch_size, num_options)\n",
    "        predicted_ranks = predicted_ranks.squeeze(1)\n",
    "        batch_size, num_options = predicted_ranks.size()\n",
    "\n",
    "        k = torch.sum(target_relevance != 0, dim=-1)\n",
    "\n",
    "        # shape: (batch_size, num_options)\n",
    "        _, rankings = torch.sort(predicted_ranks, dim=-1)\n",
    "        # Sort relevance in descending order so highest relevance gets top rnk.\n",
    "        _, best_rankings = torch.sort(\n",
    "            target_relevance, dim=-1, descending=True\n",
    "        )\n",
    "\n",
    "        # shape: (batch_size, )\n",
    "        batch_ndcg = []\n",
    "        for batch_index in range(batch_size):\n",
    "            num_relevant = k[batch_index]\n",
    "            dcg = self._dcg(\n",
    "                rankings[batch_index][:num_relevant],\n",
    "                target_relevance[batch_index],\n",
    "            )\n",
    "            best_dcg = self._dcg(\n",
    "                best_rankings[batch_index][:num_relevant],\n",
    "                target_relevance[batch_index],\n",
    "            )\n",
    "            batch_ndcg.append(dcg / best_dcg)\n",
    "\n",
    "        self._ndcg_denominator += batch_size\n",
    "        self._ndcg_numerator += sum(batch_ndcg)\n",
    "\n",
    "    def _dcg(self, rankings: torch.Tensor, relevance: torch.Tensor):\n",
    "        sorted_relevance = relevance[rankings].cpu().float()\n",
    "        discounts = torch.log2(torch.arange(len(rankings)).float() + 2)\n",
    "        return torch.sum(sorted_relevance / discounts, dim=-1)\n",
    "\n",
    "    def retrieve(self, reset: bool = True, key=\"\"):\n",
    "        if self._ndcg_denominator > 0:\n",
    "            metrics = {\n",
    "                key + \"ndcg\": float(self._ndcg_numerator / self._ndcg_denominator)\n",
    "            }\n",
    "        else:\n",
    "            metrics = {}\n",
    "\n",
    "        if reset:\n",
    "            self.reset()\n",
    "        return metrics\n",
    "\n",
    "    def reset(self):\n",
    "        self._ndcg_numerator = 0.0\n",
    "        self._ndcg_denominator = 0.0\n",
    "        \n",
    "\n",
    "def scores_to_ranks(scores: torch.Tensor):\n",
    "    \"\"\"Convert model output scores into ranks.\"\"\"\n",
    "    batch_size, num_rounds, num_options = scores.size()\n",
    "    scores = scores.view(-1, num_options)\n",
    "\n",
    "    # sort in descending order - largest score gets highest rank\n",
    "    sorted_ranks, ranked_idx = scores.sort(1, descending=True)\n",
    "\n",
    "    # i-th position in ranked_idx specifies which score shall take this\n",
    "    # position but we want i-th position to have rank of score at that\n",
    "    # position, do this conversion\n",
    "    ranks = ranked_idx.clone().fill_(0)\n",
    "    for i in range(ranked_idx.size(0)):\n",
    "        for j in range(num_options):\n",
    "            ranks[i][ranked_idx[i][j]] = j\n",
    "    # convert from 0-99 ranks to 1-100 ranks\n",
    "    ranks += 1\n",
    "    ranks = ranks.view(batch_size, num_rounds, num_options)\n",
    "    return ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15e4c2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(save_path, 'rb') as rf:\n",
    "    all_data = np.load(rf, allow_pickle=True).item()\n",
    "    all_preds = all_data['all_preds']\n",
    "    all_gt_results = all_data['all_gt_results']\n",
    "    all_losses = all_data['all_losses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7686317d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top-k, k=1, acc=0.17573\n",
      "top-k, k=5, acc=0.19971\n",
      "top-k, k=10, acc=0.24414\n",
      "top-k, k=20, acc=0.48309\n",
      "MRR: 0.21997\n",
      "NDCG: 0.16594\n"
     ]
    }
   ],
   "source": [
    "top_k_accuracy = collections.defaultdict(list)\n",
    "mrr_results = []\n",
    "all_ranks = []\n",
    "topk = (1, 5, 10, 20)\n",
    "ndcg = NDCG()\n",
    "\n",
    "assert len(all_preds) == len(all_gt_results)\n",
    "for gt, loss in zip(all_gt_results, all_losses):\n",
    "    scores = -loss\n",
    "    _, preds = torch.tensor(scores).topk(100)\n",
    "    rank = np.where(preds == gt)[0][0] + 1\n",
    "    all_ranks.append(rank)\n",
    "    mrr_results.append(1 / rank)\n",
    "\n",
    "    for k in topk:\n",
    "        acc = gt in preds[:k]\n",
    "        top_k_accuracy[k].append(acc)\n",
    "        \n",
    "dense_mrr = []\n",
    "for i in range(len(dense_data)):\n",
    "    idx = i * 10 + dense_data[i]['round_id']\n",
    "    if idx >= len(all_losses):\n",
    "        break\n",
    "    scores = -torch.tensor(all_losses[idx])[None, :]\n",
    "    relevance = torch.tensor(dense_data[i]['gt_relevance'])[None, :]\n",
    "    ndcg.observe(scores, relevance)\n",
    "    dense_mrr.append(mrr_results[idx])\n",
    "\n",
    "for k in topk:\n",
    "    print(f'top-k, k={k}, acc={np.mean(top_k_accuracy[k]):.5f}')\n",
    "print(f'MRR: {np.mean(mrr_results):.5f}')\n",
    "print(f'NDCG: {ndcg.retrieve(reset=True)[\"ndcg\"]:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1982d6fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
