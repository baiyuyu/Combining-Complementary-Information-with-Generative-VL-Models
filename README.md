# Combining-Complementary-Multimodal-Information-in-Visual-Language-Models
# Multimodal Integration Analysis: Code Repository

This repository contains the analysis scripts for the public version of our paper:

**If I Feel Smart, I Will Do the Right Thing: Combining Complementary Multimodal Information in Visual Language Models**

**Yuyu Bai & Sandro Pezzelle** 

 
**Vrije Universiteit Amsterdam**  

## Abstract

Generative visual language models (VLMs) have recently shown potential across various downstream language-and-vision tasks. At the same time, it is still an open question whether, and to what extent, these models can 
properly understand a multimodal context where language and vision provide complementary information---a mechanism routinely in place in human language communication. In this work, we test various VLMs on the task of generating action descriptions consistent with both an image's visual content and an intention or attitude (not visually grounded) conveyed by a textual prompt. Our results show that BLIP-2 is not far from human performance when the task is framed as a generative multiple-choice problem, while other models struggle. Furthermore, the actions generated by BLIP-2 in an open-ended generative setting are better than those by the competitors; indeed, human annotators judge most of them as plausible continuations for the multimodal context. Our study reveals substantial variability among VLMs in integrating complementary multimodal information, yet BLIP-2 demonstrates promising trends across most evaluations, paving the way for seamless human-computer interaction.

---

## Features

- Scripts for data analysis, visualization, and experiment.
- Reproducible analysis for the experiments discussed in the paper.

---

## Getting Started

### Prerequisites

- Python >= 3.8
- Required Python libraries:
  - `pandas`
  - `numpy`
  - `matplotlib` / `seaborn`
  - `Transformer`
  - [Other libraries, e.g., `scikit-learn`, if applicable]

### Install dependencies:
```bash
pip install -r requirements.txt
```

### Installation
Clone this repository:

git clone https://github.com/your_username/repo_name.git
cd repo_name
Install the required dependencies:
pip install -r requirements.txt

### Data Preparation
Download the dataset(s) from [https://sites.google.com/view/bd2bb].
Place the dataset(s) in the data/ directory.


### Repository Structure

repo_name/
│
├── mapl/             # Scripts for conducting 2 experiments with the MAPL model.
├── fromage/          # Scripts for running 2 experiments with the Fromage model.
├── idefics/          # Scripts for running 2 experiments with the Idefics model.
├── blip2/            # Scripts for conducting 2 experiments and performing data analysis for the BLIP2 model.
├── requirements.txt  # List of dependencies.
└── README.md         # Main documentation for the repository, including setup, usage instructions, and references.





### Citation
If you use this code, please cite our paper:

@article{your_paper_citation,
  title={Your Paper Title},
  author={Your Name and Co-authors},
  journal={Journal/Conference Name},
  year={2024},
  url={link to the paper}
}

